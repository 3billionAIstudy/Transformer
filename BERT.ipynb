{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Title\n",
    "[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
    "](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "## Authors and Year\n",
    "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova (2018)\n",
    "\n",
    "## Abstract\n",
    "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n",
    "BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1.\n",
    "\n",
    "## Model Type\n",
    "Transformer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "- BERT alleviates the previously mentioned unidirectionality constraint by using a “masked language model” (MLM) pre-training objective.    \n",
    "- BERT uses masked language models to enable pretrained deep bidirectional representations.    \n",
    "- We show that pre-trained representations reduce the need for many heavily-engineered task specific architectures."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-training and fine-tuning for BERT\n",
    "- During pre-training, the model is trained on unlabeled data over different pre-training tasks.\n",
    "- For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Images/BERT/BERT_fig1.png\" alt=\"drawing\" width=\"700\"/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input representation\n",
    "- Input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., h Question, Answeri) in one token sequence.\n",
    "- A “sequence” refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.\n",
    "- For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings.\n",
    "  - Token Embeddings: [WordPiece Embeddings](https://ai.googleblog.com/2021/12/a-fast-wordpiece-tokenization-system.html)\n",
    "  - Segment Embeddings: A learned embedding to every token indicating whether it belongs to sentence A or sentence B.\n",
    "  - [Position Embeddings](https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding): 트랜스포머는 시퀀스 데이터를 순차적으로 입력 받지 않기 때문에 단어의 순서를 인지하기 위한 장치가 필요함\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Images/BERT/BERT_fig2.png\" alt=\"drawing\" width=\"700\"/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-training BERT\n",
    "<p align=\"center\">\n",
    "    <img src=\"Images/BERT/BERT_fig3.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "### 1. Maked LM\n",
    "- In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens.\n",
    "- 15% of all WordPiece tokens were masked in each sequence at random.\n",
    "- The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Images/BERT/Masked_LM_2.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "출처: 딥 러닝을 이용한 자연어 처리 입문\n",
    "\n",
    "### 2. Next Sentence Prediction (NSP)\n",
    "- In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus.\n",
    "- when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext).\n",
    "- The [CLS] token is used for next sentence prediction (NSP).\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Images/BERT/NSP.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "출처: 딥 러닝을 이용한 자연어 처리 입문\n",
    "\n",
    "These two pre-training methods were applied simultanously. Removing one of them made the performance worse."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine-tuning BERT on different tasks\n",
    "<p align=\"center\">\n",
    "    <img src=\"Images/BERT/BERT_fig4.png\" alt=\"drawing\" width=\"700\"/>"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}